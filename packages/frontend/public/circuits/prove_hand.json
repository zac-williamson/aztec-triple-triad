{"noir_version":"1.0.0-beta.18+2db78f8894936db05c53430f364360ac9cc5c61f","hash":"8662042179468661353","abi":{"parameters":[{"name":"card_commit_hash","type":{"kind":"field"},"visibility":"public"},{"name":"card_ids","type":{"kind":"array","length":5,"type":{"kind":"field"}},"visibility":"private"},{"name":"blinding_factor","type":{"kind":"field"},"visibility":"private"}],"return_type":null,"error_types":{"13556716706216405166":{"error_kind":"string","string":"Card commitment mismatch"},"16351927787417371723":{"error_kind":"string","string":"Card ID must be >= 1"},"16493610627926674244":{"error_kind":"string","string":"Card ID must be <= 50"},"17038174891351532373":{"error_kind":"string","string":"Duplicate card IDs not allowed"}}},"bytecode":"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","debug_symbols":"pZXNjuowDIXfJWsWcZwfh1e5ukIFyqhSVVCnHekKzbvfkJN0YFEWnY2P2+Cjz05K7urcHuePQzdcrp9q/+eujmPX993Hob+emqm7Dunt/Xun6uNhGts2vVJP66nq1oztMKn9MPf9Tn01/Zx/9HlrhqxTM6ZVvVPtcE6aDC9d3z6y791PtV4vtRRDqbZGy2LgaJMDuS0Ohs3iwHGTg9DiIJscLMfqYJ3e5GDsLx0CLw7B05pDWHdgy8WA5Wkv40u9vKnXZjHQTFscSPvqQNauOdDbMURXxyBsFgva1oY3W9rwrjKw97zJwejFgVcZ3s4hLMdB9Ouh/puemlM3vvydKK326UOiHE2OnKPN0eXocww5So4RVaU4VafZkYGk+sRAFuIgHpI80laTQGIWoyEEMRCGWAg4DEAMSAxQDFgYLIxOuLSCXhjNMFzYZ04OEMlkHLNYDSEIWCxYLFisg3gIOrJgsWBxYHFgcWBxYHFlsGBx6MihIwcXFzOZ1xCweAPBdD2m68HiweLB4jFdD5YAlgCWAJYAlgCWAJZQthksASwBLgIWwU4LWAQsAhbBTgtYBCwCF4FLBEsESwRLBEsESwRLBEsESyyHDnMhrYtiMqRN0XLydDl6OnkRPRJfk1ATqUksyeMsI0mu+Ur6asauOfZtuXUv83B6uoSnf7e6Uq/p23g9ted5bB9fWF5L39x/","file_map":{"19":{"source":"// Exposed only for usage in `std::meta`\npub(crate) mod poseidon2;\n\nuse crate::default::Default;\nuse crate::embedded_curve_ops::{\n    EmbeddedCurvePoint, EmbeddedCurveScalar, multi_scalar_mul, multi_scalar_mul_array_return,\n};\nuse crate::meta::derive_via;\n\n#[foreign(sha256_compression)]\n// docs:start:sha256_compression\npub fn sha256_compression(input: [u32; 16], state: [u32; 8]) -> [u32; 8] {}\n// docs:end:sha256_compression\n\n#[foreign(keccakf1600)]\n// docs:start:keccakf1600\npub fn keccakf1600(input: [u64; 25]) -> [u64; 25] {}\n// docs:end:keccakf1600\n\npub mod keccak {\n    #[deprecated(\"This function has been moved to std::hash::keccakf1600\")]\n    pub fn keccakf1600(input: [u64; 25]) -> [u64; 25] {\n        super::keccakf1600(input)\n    }\n}\n\n#[foreign(blake2s)]\n// docs:start:blake2s\npub fn blake2s<let N: u32>(input: [u8; N]) -> [u8; 32]\n// docs:end:blake2s\n{}\n\n// docs:start:blake3\npub fn blake3<let N: u32>(input: [u8; N]) -> [u8; 32]\n// docs:end:blake3\n{\n    if crate::runtime::is_unconstrained() {\n        // Temporary measure while Barretenberg is main proving system.\n        // Please open an issue if you're working on another proving system and running into problems due to this.\n        crate::static_assert(\n            N <= 1024,\n            \"Barretenberg cannot prove blake3 hashes with inputs larger than 1024 bytes\",\n        );\n    }\n    __blake3(input)\n}\n\n#[foreign(blake3)]\nfn __blake3<let N: u32>(input: [u8; N]) -> [u8; 32] {}\n\n// docs:start:pedersen_commitment\npub fn pedersen_commitment<let N: u32>(input: [Field; N]) -> EmbeddedCurvePoint {\n    // docs:end:pedersen_commitment\n    pedersen_commitment_with_separator(input, 0)\n}\n\n#[inline_always]\npub fn pedersen_commitment_with_separator<let N: u32>(\n    input: [Field; N],\n    separator: u32,\n) -> EmbeddedCurvePoint {\n    let mut points = [EmbeddedCurveScalar { lo: 0, hi: 0 }; N];\n    for i in 0..N {\n        // we use the unsafe version because the multi_scalar_mul will constrain the scalars.\n        points[i] = from_field_unsafe(input[i]);\n    }\n    let generators = derive_generators(\"DEFAULT_DOMAIN_SEPARATOR\".as_bytes(), separator);\n    multi_scalar_mul(generators, points)\n}\n\n// docs:start:pedersen_hash\npub fn pedersen_hash<let N: u32>(input: [Field; N]) -> Field\n// docs:end:pedersen_hash\n{\n    pedersen_hash_with_separator(input, 0)\n}\n\n#[no_predicates]\npub fn pedersen_hash_with_separator<let N: u32>(input: [Field; N], separator: u32) -> Field {\n    let mut scalars: [EmbeddedCurveScalar; N + 1] = [EmbeddedCurveScalar { lo: 0, hi: 0 }; N + 1];\n    let mut generators: [EmbeddedCurvePoint; N + 1] =\n        [EmbeddedCurvePoint::point_at_infinity(); N + 1];\n    crate::assert_constant(separator);\n    let domain_generators: [EmbeddedCurvePoint; N] =\n        derive_generators(\"DEFAULT_DOMAIN_SEPARATOR\".as_bytes(), separator);\n\n    for i in 0..N {\n        scalars[i] = from_field_unsafe(input[i]);\n        generators[i] = domain_generators[i];\n    }\n    scalars[N] = EmbeddedCurveScalar { lo: N as Field, hi: 0 as Field };\n\n    let length_generator: [EmbeddedCurvePoint; 1] =\n        derive_generators(\"pedersen_hash_length\".as_bytes(), 0);\n    generators[N] = length_generator[0];\n    multi_scalar_mul_array_return(generators, scalars, true)[0].x\n}\n\n#[field(bn254)]\n#[inline_always]\npub fn derive_generators<let N: u32, let M: u32>(\n    domain_separator_bytes: [u8; M],\n    starting_index: u32,\n) -> [EmbeddedCurvePoint; N] {\n    crate::assert_constant(domain_separator_bytes);\n    crate::assert_constant(starting_index);\n    __derive_generators(domain_separator_bytes, starting_index)\n}\n\n#[builtin(derive_pedersen_generators)]\n#[field(bn254)]\nfn __derive_generators<let N: u32, let M: u32>(\n    domain_separator_bytes: [u8; M],\n    starting_index: u32,\n) -> [EmbeddedCurvePoint; N] {}\n\n#[field(bn254)]\n// Decompose the input 'bn254 scalar' into two 128 bits limbs.\n// It is called 'unsafe' because it does not assert the limbs are 128 bits\n// Assuming the limbs are 128 bits:\n// Assert the decomposition does not overflow the field size.\nfn from_field_unsafe(scalar: Field) -> EmbeddedCurveScalar {\n    // Safety: xlo and xhi decomposition is checked below\n    let (xlo, xhi) = unsafe { crate::field::bn254::decompose_hint(scalar) };\n    // Check that the decomposition is correct\n    assert_eq(scalar, xlo + crate::field::bn254::TWO_POW_128 * xhi);\n    // Check that the decomposition does not overflow the field size\n    let (a, b) = if xhi == crate::field::bn254::PHI {\n        (xlo, crate::field::bn254::PLO)\n    } else {\n        (xhi, crate::field::bn254::PHI)\n    };\n    crate::field::bn254::assert_lt(a, b);\n\n    EmbeddedCurveScalar { lo: xlo, hi: xhi }\n}\n\npub fn poseidon2_permutation<let N: u32>(input: [Field; N], state_len: u32) -> [Field; N] {\n    assert_eq(input.len(), state_len);\n    poseidon2_permutation_internal(input)\n}\n\n#[foreign(poseidon2_permutation)]\nfn poseidon2_permutation_internal<let N: u32>(input: [Field; N]) -> [Field; N] {}\n\n// Generic hashing support.\n// Partially ported and impacted by rust.\n\n// Hash trait shall be implemented per type.\n#[derive_via(derive_hash)]\npub trait Hash {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher;\n}\n\n// docs:start:derive_hash\ncomptime fn derive_hash(s: TypeDefinition) -> Quoted {\n    let name = quote { $crate::hash::Hash };\n    let signature = quote { fn hash<H>(_self: Self, _state: &mut H) where H: $crate::hash::Hasher };\n    let for_each_field = |name| quote { _self.$name.hash(_state); };\n    crate::meta::make_trait_impl(\n        s,\n        name,\n        signature,\n        for_each_field,\n        quote {},\n        |fields| fields,\n    )\n}\n// docs:end:derive_hash\n\n// Hasher trait shall be implemented by algorithms to provide hash-agnostic means.\n// TODO: consider making the types generic here ([u8], [Field], etc.)\npub trait Hasher {\n    fn finish(self) -> Field;\n\n    fn write(&mut self, input: Field);\n}\n\n// BuildHasher is a factory trait, responsible for production of specific Hasher.\npub trait BuildHasher {\n    type H: Hasher;\n\n    fn build_hasher(self) -> H;\n}\n\npub struct BuildHasherDefault<H>;\n\nimpl<H> BuildHasher for BuildHasherDefault<H>\nwhere\n    H: Hasher + Default,\n{\n    type H = H;\n\n    fn build_hasher(_self: Self) -> H {\n        H::default()\n    }\n}\n\nimpl<H> Default for BuildHasherDefault<H>\nwhere\n    H: Hasher + Default,\n{\n    fn default() -> Self {\n        BuildHasherDefault {}\n    }\n}\n\nimpl Hash for Field {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self);\n    }\n}\n\nimpl Hash for u1 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u8 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u16 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u32 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u64 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for u128 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for i8 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as u8 as Field);\n    }\n}\n\nimpl Hash for i16 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as u16 as Field);\n    }\n}\n\nimpl Hash for i32 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as u32 as Field);\n    }\n}\n\nimpl Hash for i64 {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as u64 as Field);\n    }\n}\n\nimpl Hash for bool {\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        H::write(state, self as Field);\n    }\n}\n\nimpl Hash for () {\n    fn hash<H>(_self: Self, _state: &mut H)\n    where\n        H: Hasher,\n    {}\n}\n\nimpl<T, let N: u32> Hash for [T; N]\nwhere\n    T: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        for elem in self {\n            elem.hash(state);\n        }\n    }\n}\n\nimpl<T> Hash for [T]\nwhere\n    T: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.len().hash(state);\n        for elem in self {\n            elem.hash(state);\n        }\n    }\n}\n\nimpl<A, B> Hash for (A, B)\nwhere\n    A: Hash,\n    B: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.0.hash(state);\n        self.1.hash(state);\n    }\n}\n\nimpl<A, B, C> Hash for (A, B, C)\nwhere\n    A: Hash,\n    B: Hash,\n    C: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.0.hash(state);\n        self.1.hash(state);\n        self.2.hash(state);\n    }\n}\n\nimpl<A, B, C, D> Hash for (A, B, C, D)\nwhere\n    A: Hash,\n    B: Hash,\n    C: Hash,\n    D: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.0.hash(state);\n        self.1.hash(state);\n        self.2.hash(state);\n        self.3.hash(state);\n    }\n}\n\nimpl<A, B, C, D, E> Hash for (A, B, C, D, E)\nwhere\n    A: Hash,\n    B: Hash,\n    C: Hash,\n    D: Hash,\n    E: Hash,\n{\n    fn hash<H>(self, state: &mut H)\n    where\n        H: Hasher,\n    {\n        self.0.hash(state);\n        self.1.hash(state);\n        self.2.hash(state);\n        self.3.hash(state);\n        self.4.hash(state);\n    }\n}\n\n// Some test vectors for Pedersen hash and Pedersen Commitment.\n// They have been generated using the same functions so the tests are for now useless\n// but they will be useful when we switch to Noir implementation.\n#[test]\nfn assert_pedersen() {\n    assert_eq(\n        pedersen_hash_with_separator([1], 1),\n        0x1b3f4b1a83092a13d8d1a59f7acb62aba15e7002f4440f2275edb99ebbc2305f,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1], 1),\n        EmbeddedCurvePoint {\n            x: 0x054aa86a73cb8a34525e5bbed6e43ba1198e860f5f3950268f71df4591bde402,\n            y: 0x209dcfbf2cfb57f9f6046f44d71ac6faf87254afc7407c04eb621a6287cac126,\n            is_infinite: false,\n        },\n    );\n\n    assert_eq(\n        pedersen_hash_with_separator([1, 2], 2),\n        0x26691c129448e9ace0c66d11f0a16d9014a9e8498ee78f4d69f0083168188255,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2], 2),\n        EmbeddedCurvePoint {\n            x: 0x2e2b3b191e49541fe468ec6877721d445dcaffe41728df0a0eafeb15e87b0753,\n            y: 0x2ff4482400ad3a6228be17a2af33e2bcdf41be04795f9782bd96efe7e24f8778,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3], 3),\n        0x0bc694b7a1f8d10d2d8987d07433f26bd616a2d351bc79a3c540d85b6206dbe4,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3], 3),\n        EmbeddedCurvePoint {\n            x: 0x1fee4e8cf8d2f527caa2684236b07c4b1bad7342c01b0f75e9a877a71827dc85,\n            y: 0x2f9fedb9a090697ab69bf04c8bc15f7385b3e4b68c849c1536e5ae15ff138fd1,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4], 4),\n        0xdae10fb32a8408521803905981a2b300d6a35e40e798743e9322b223a5eddc,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4], 4),\n        EmbeddedCurvePoint {\n            x: 0x07ae3e202811e1fca39c2d81eabe6f79183978e6f12be0d3b8eda095b79bdbc9,\n            y: 0x0afc6f892593db6fbba60f2da558517e279e0ae04f95758587760ba193145014,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5], 5),\n        0xfc375b062c4f4f0150f7100dfb8d9b72a6d28582dd9512390b0497cdad9c22,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5], 5),\n        EmbeddedCurvePoint {\n            x: 0x1754b12bd475a6984a1094b5109eeca9838f4f81ac89c5f0a41dbce53189bb29,\n            y: 0x2da030e3cfcdc7ddad80eaf2599df6692cae0717d4e9f7bfbee8d073d5d278f7,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6], 6),\n        0x1696ed13dc2730062a98ac9d8f9de0661bb98829c7582f699d0273b18c86a572,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6], 6),\n        EmbeddedCurvePoint {\n            x: 0x190f6c0e97ad83e1e28da22a98aae156da083c5a4100e929b77e750d3106a697,\n            y: 0x1f4b60f34ef91221a0b49756fa0705da93311a61af73d37a0c458877706616fb,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6, 7], 7),\n        0x128c0ff144fc66b6cb60eeac8a38e23da52992fc427b92397a7dffd71c45ede3,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6, 7], 7),\n        EmbeddedCurvePoint {\n            x: 0x015441e9d29491b06563fac16fc76abf7a9534c715421d0de85d20dbe2965939,\n            y: 0x1d2575b0276f4e9087e6e07c2cb75aa1baafad127af4be5918ef8a2ef2fea8fc,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6, 7, 8], 8),\n        0x2f960e117482044dfc99d12fece2ef6862fba9242be4846c7c9a3e854325a55c,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6, 7, 8], 8),\n        EmbeddedCurvePoint {\n            x: 0x1657737676968887fceb6dd516382ea13b3a2c557f509811cd86d5d1199bc443,\n            y: 0x1f39f0cb569040105fa1e2f156521e8b8e08261e635a2b210bdc94e8d6d65f77,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6, 7, 8, 9], 9),\n        0x0c96db0790602dcb166cc4699e2d306c479a76926b81c2cb2aaa92d249ec7be7,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6, 7, 8, 9], 9),\n        EmbeddedCurvePoint {\n            x: 0x0a3ceae42d14914a432aa60ec7fded4af7dad7dd4acdbf2908452675ec67e06d,\n            y: 0xfc19761eaaf621ad4aec9a8b2e84a4eceffdba78f60f8b9391b0bd9345a2f2,\n            is_infinite: false,\n        },\n    );\n    assert_eq(\n        pedersen_hash_with_separator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 10),\n        0x2cd37505871bc460a62ea1e63c7fe51149df5d0801302cf1cbc48beb8dff7e94,\n    );\n    assert_eq(\n        pedersen_commitment_with_separator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 10),\n        EmbeddedCurvePoint {\n            x: 0x2fb3f8b3d41ddde007c8c3c62550f9a9380ee546fcc639ffbb3fd30c8d8de30c,\n            y: 0x300783be23c446b11a4c0fabf6c91af148937cea15fcf5fb054abf7f752ee245,\n            is_infinite: false,\n        },\n    );\n}\n","path":"std/hash/mod.nr"},"51":{"source":"/// Prove Hand circuit.\n/// Proves that a player owns 5 specific cards without revealing which ones.\n/// Commits to card IDs using poseidon2 hash with a blinding factor.\n///\n/// Public inputs:\n///   card_commit_hash - poseidon2 hash commitment of the player's hand\n///\n/// Private inputs:\n///   card_ids - the 5 card token IDs\n///   blinding_factor - blinding factor (e.g. poseidon2_hash([nhk_app_secret, contract_address]))\n///\n/// Commitment format (6 fields):\n///   poseidon2_hash([card_ids[0], card_ids[1], card_ids[2], card_ids[3], card_ids[4], blinding_factor])\n\nuse poseidon::poseidon2::Poseidon2;\n\n/// Hardcoded card rank database: 50 cards with ranks [top, right, bottom, left].\n/// Index 0 is unused (card IDs are 1-50).\nfn get_card_ranks(card_id: u32) -> [Field; 4] {\n    // Card ranks: [top, right, bottom, left]\n    // Index 0 = dummy for 1-based card IDs\n    assert((card_id >= 1) & (card_id <= 50), \"Invalid card ID for rank lookup\");\n\n    // Level 1 - Common (IDs 1-10)\n    if card_id == 1 { [1, 4, 1, 5] }        // Mudwalker\n    else if card_id == 2 { [5, 1, 1, 3] }    // Blushy\n    else if card_id == 3 { [1, 3, 3, 5] }    // Snowdrop\n    else if card_id == 4 { [6, 1, 1, 2] }    // Sunny\n    else if card_id == 5 { [2, 3, 1, 5] }    // Inkwell\n    else if card_id == 6 { [2, 1, 4, 4] }    // Stripes\n    else if card_id == 7 { [1, 5, 4, 1] }    // Barkeeper\n    else if card_id == 8 { [3, 1, 5, 2] }    // Dotty\n    else if card_id == 9 { [2, 1, 6, 1] }    // Penny\n    else if card_id == 10 { [4, 3, 2, 4] }   // Peaches\n    // Level 2 - Uncommon (IDs 11-20)\n    else if card_id == 11 { [2, 6, 1, 6] }   // Freckles\n    else if card_id == 12 { [7, 1, 3, 1] }   // Camo\n    else if card_id == 13 { [6, 2, 2, 3] }   // Neon\n    else if card_id == 14 { [5, 3, 3, 4] }   // Glow Bug\n    else if card_id == 15 { [6, 1, 4, 3] }   // Limelight\n    else if card_id == 16 { [3, 4, 5, 3] }   // Marble\n    else if card_id == 17 { [5, 3, 2, 5] }   // Sapphire\n    else if card_id == 18 { [5, 1, 3, 5] }   // Jefferson\n    else if card_id == 19 { [5, 2, 5, 2] }   // Longfoot\n    else if card_id == 20 { [4, 2, 4, 5] }   // Featherfin\n    // Level 3 - Rare (IDs 21-30)\n    else if card_id == 21 { [3, 7, 2, 5] }   // Lilac\n    else if card_id == 22 { [5, 2, 5, 5] }   // Patches\n    else if card_id == 23 { [6, 6, 3, 3] }   // Faded\n    else if card_id == 24 { [6, 3, 6, 3] }   // Gold Dust\n    else if card_id == 25 { [3, 5, 5, 5] }   // Phantom\n    else if card_id == 26 { [7, 5, 1, 3] }   // Ash\n    else if card_id == 27 { [7, 1, 5, 3] }   // Cocoa\n    else if card_id == 28 { [5, 3, 6, 3] }   // Ringmaster\n    else if card_id == 29 { [5, 6, 2, 4] }   // Goldrush\n    else if card_id == 30 { [4, 4, 7, 2] }   // Swampling\n    // Level 4 - Epic (IDs 31-40)\n    else if card_id == 31 { [3, 6, 4, 7] }   // Glitter\n    else if card_id == 32 { [7, 2, 3, 7] }   // Starfield\n    else if card_id == 33 { [2, 3, 7, 7] }   // Specter\n    else if card_id == 34 { [6, 5, 5, 5] }   // Saffron\n    else if card_id == 35 { [4, 7, 6, 2] }   // Stardust\n    else if card_id == 36 { [2, 3, 7, 8] }   // Achoque\n    else if card_id == 37 { [1, 7, 6, 4] }   // Zacapu\n    else if card_id == 38 { [7, 3, 1, 6] }   // Laguna\n    else if card_id == 39 { [7, 4, 4, 4] }   // Streamwalker\n    else if card_id == 40 { [3, 7, 3, 6] }   // Digger\n    // Level 5 - Legendary (IDs 41-50)\n    else if card_id == 41 { [6, 7, 3, 7] }   // Eclipse\n    else if card_id == 42 { [6, 5, 8, 4] }   // Kaleidoscope\n    else if card_id == 43 { [6, 5, 6, 6] }   // Twinned\n    else if card_id == 44 { [3, 6, 7, 8] }   // Sparkletail\n    else if card_id == 45 { [7, 6, 5, 6] }   // Riddler\n    else if card_id == 46 { [3, 10, 2, 1] }  // Rosita\n    else if card_id == 47 { [6, 2, 6, 7] }   // Brooklet\n    else if card_id == 48 { [5, 5, 7, 6] }   // Whisper\n    else if card_id == 49 { [7, 7, 4, 2] }   // Misty\n    else { [7, 2, 7, 4] }                    // Lerma (card_id == 50)\n}\n\nfn main(\n    // Public inputs\n    card_commit_hash: pub Field,\n    // Private inputs\n    card_ids: [Field; 5],\n    blinding_factor: Field,\n) {\n    // 1. Verify all card IDs are valid (1-50) and unique\n    for i in 0..5 {\n        let id = card_ids[i] as u32;\n        assert(id >= 1, \"Card ID must be >= 1\");\n        assert(id <= 50, \"Card ID must be <= 50\");\n\n        // Check uniqueness against all subsequent cards\n        for j in (i + 1)..5 {\n            assert(card_ids[i] != card_ids[j], \"Duplicate card IDs not allowed\");\n        }\n    }\n\n    // 2. Compute commitment and verify it matches the public input\n    let mut to_hash: [Field; 6] = [0; 6];\n    for i in 0..5 {\n        to_hash[i] = card_ids[i];\n    }\n    to_hash[5] = blinding_factor;\n    let computed_commit = Poseidon2::hash(to_hash, 6);\n    assert(computed_commit == card_commit_hash, \"Card commitment mismatch\");\n}\n\n// ====================== TESTS ======================\n\n/// Helper to compute card_commit_hash in tests\nfn test_compute_commit(card_ids: [Field; 5], blinding_factor: Field) -> Field {\n    let mut to_hash: [Field; 6] = [0; 6];\n    for i in 0..5 {\n        to_hash[i] = card_ids[i];\n    }\n    to_hash[5] = blinding_factor;\n    Poseidon2::hash(to_hash, 6)\n}\n\n#[test]\nfn test_prove_hand() {\n    let card_ids: [Field; 5] = [1, 2, 3, 4, 5];\n    let blinding_factor: Field = 12345;\n\n    let commit = test_compute_commit(card_ids, blinding_factor);\n\n    main(commit, card_ids, blinding_factor);\n}\n\n#[test]\nfn test_prove_hand_high_card_ids() {\n    let card_ids: [Field; 5] = [46, 47, 48, 49, 50];\n    let blinding_factor: Field = 99999;\n\n    let commit = test_compute_commit(card_ids, blinding_factor);\n\n    main(commit, card_ids, blinding_factor);\n}\n\n#[test]\nfn test_prove_hand_level4_cards() {\n    let card_ids: [Field; 5] = [31, 32, 33, 34, 35];\n    let blinding_factor: Field = 55555;\n\n    let commit = test_compute_commit(card_ids, blinding_factor);\n\n    main(commit, card_ids, blinding_factor);\n}\n\n#[test]\nfn test_prove_hand_mixed_levels() {\n    let card_ids: [Field; 5] = [1, 15, 25, 35, 50];\n    let blinding_factor: Field = 777;\n\n    let commit = test_compute_commit(card_ids, blinding_factor);\n\n    main(commit, card_ids, blinding_factor);\n}\n\n#[test(should_fail_with = \"Card ID must be >= 1\")]\nfn test_fail_card_id_zero() {\n    let card_ids: [Field; 5] = [0, 2, 3, 4, 5];\n    let blinding_factor: Field = 12345;\n    let commit = test_compute_commit(card_ids, blinding_factor);\n\n    main(commit, card_ids, blinding_factor);\n}\n\n#[test(should_fail_with = \"Card ID must be <= 50\")]\nfn test_fail_card_id_too_high() {\n    let card_ids: [Field; 5] = [1, 2, 3, 4, 51];\n    let blinding_factor: Field = 12345;\n    let commit = test_compute_commit(card_ids, blinding_factor);\n\n    main(commit, card_ids, blinding_factor);\n}\n\n#[test(should_fail_with = \"Duplicate card IDs not allowed\")]\nfn test_fail_duplicate_card_ids() {\n    let card_ids: [Field; 5] = [1, 2, 3, 4, 1];\n    let blinding_factor: Field = 12345;\n    let commit = test_compute_commit(card_ids, blinding_factor);\n\n    main(commit, card_ids, blinding_factor);\n}\n\n#[test(should_fail_with = \"Card commitment mismatch\")]\nfn test_fail_wrong_commitment() {\n    let card_ids: [Field; 5] = [1, 2, 3, 4, 5];\n    let blinding_factor: Field = 12345;\n    let wrong_commit: Field = 0xdeadbeef;\n\n    main(wrong_commit, card_ids, blinding_factor);\n}\n\n#[test(should_fail_with = \"Card commitment mismatch\")]\nfn test_fail_wrong_blinding_factor() {\n    let card_ids: [Field; 5] = [1, 2, 3, 4, 5];\n    let blinding_factor: Field = 12345;\n    let commit = test_compute_commit(card_ids, blinding_factor);\n\n    // Use wrong blinding factor\n    main(commit, card_ids, 99999);\n}\n","path":"/Users/zac/aztec-triple-triad-ui/repo/circuits/prove_hand/src/main.nr"},"59":{"source":"use std::default::Default;\nuse std::hash::Hasher;\n\nglobal RATE: u32 = 3;\n\npub struct Poseidon2 {\n    cache: [Field; 3],\n    state: [Field; 4],\n    cache_size: u32,\n    squeeze_mode: bool, // 0 => absorb, 1 => squeeze\n}\n\nimpl Poseidon2 {\n    #[no_predicates]\n    pub fn hash<let N: u32>(input: [Field; N], message_size: u32) -> Field {\n        Poseidon2::hash_internal(input, message_size)\n    }\n\n    pub(crate) fn new(iv: Field) -> Poseidon2 {\n        let mut result =\n            Poseidon2 { cache: [0; 3], state: [0; 4], cache_size: 0, squeeze_mode: false };\n        result.state[RATE] = iv;\n        result\n    }\n\n    fn perform_duplex(&mut self) {\n        // add the cache into sponge state\n        self.state[0] += self.cache[0];\n        self.state[1] += self.cache[1];\n        self.state[2] += self.cache[2];\n        self.state = crate::poseidon2_permutation(self.state, 4);\n    }\n\n    fn absorb(&mut self, input: Field) {\n        assert(!self.squeeze_mode);\n        if self.cache_size == RATE {\n            // If we're absorbing, and the cache is full, apply the sponge permutation to compress the cache\n            self.perform_duplex();\n            self.cache[0] = input;\n            self.cache_size = 1;\n        } else {\n            // If we're absorbing, and the cache is not full, add the input into the cache\n            self.cache[self.cache_size] = input;\n            self.cache_size += 1;\n        }\n    }\n\n    fn squeeze(&mut self) -> Field {\n        assert(!self.squeeze_mode);\n        // If we're in absorb mode, apply sponge permutation to compress the cache.\n        self.perform_duplex();\n        self.squeeze_mode = true;\n\n        // Pop one item off the top of the permutation and return it.\n        self.state[0]\n    }\n\n    fn hash_internal<let N: u32>(input: [Field; N], in_len: u32) -> Field {\n        let two_pow_64 = 18446744073709551616;\n        let iv: Field = (in_len as Field) * two_pow_64;\n        let mut state = [0; 4];\n        state[RATE] = iv;\n\n        if std::runtime::is_unconstrained() {\n            for i in 0..(in_len / RATE) {\n                state[0] += input[i * RATE];\n                state[1] += input[i * RATE + 1];\n                state[2] += input[i * RATE + 2];\n                state = crate::poseidon2_permutation(state, 4);\n            }\n\n            // handle remaining elements after last full RATE-sized chunk\n            let num_extra_fields = in_len % RATE;\n            if num_extra_fields != 0 {\n                let remainder_start = in_len - num_extra_fields;\n                state[0] += input[remainder_start];\n                if num_extra_fields > 1 {\n                    state[1] += input[remainder_start + 1];\n                }\n            }\n        } else {\n            let mut states: [[Field; 4]; N / RATE + 1] = [[0; 4]; N / RATE + 1];\n            states[0] = state;\n\n            // process all full RATE-sized chunks, storing state after each permutation\n            for chunk_idx in 0..(N / RATE) {\n                for i in 0..RATE {\n                    state[i] += input[chunk_idx * RATE + i];\n                }\n                state = crate::poseidon2_permutation(state, 4);\n                states[chunk_idx + 1] = state;\n            }\n\n            // get state at the last full block before in_len\n            let first_partially_filled_chunk = in_len / RATE;\n            state = states[first_partially_filled_chunk];\n\n            // handle remaining elements after last full RATE-sized chunk\n            let remainder_start = (in_len / RATE) * RATE;\n            for j in 0..RATE {\n                let idx = remainder_start + j;\n                if idx < in_len {\n                    state[j] += input[idx];\n                }\n            }\n        }\n\n        // always run final permutation unless we just completed a full chunk\n        // still need to permute once if in_len is 0\n        if (in_len == 0) | (in_len % RATE != 0) {\n            state = crate::poseidon2_permutation(state, 4);\n        };\n\n        state[0]\n    }\n}\n\npub struct Poseidon2Hasher {\n    _state: [Field],\n}\n\nimpl Hasher for Poseidon2Hasher {\n    fn finish(self) -> Field {\n        let iv: Field = (self._state.len() as Field) * 18446744073709551616; // iv = (self._state.len() << 64)\n        let mut sponge = Poseidon2::new(iv);\n        for i in 0..self._state.len() {\n            sponge.absorb(self._state[i]);\n        }\n        sponge.squeeze()\n    }\n\n    fn write(&mut self, input: Field) {\n        self._state = self._state.push_back(input);\n    }\n}\n\nimpl Default for Poseidon2Hasher {\n    fn default() -> Self {\n        Poseidon2Hasher { _state: &[] }\n    }\n}\n","path":"/Users/zac/nargo/github.com/noir-lang/poseidon/v0.2.6/src/poseidon2.nr"}}}